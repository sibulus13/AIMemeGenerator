{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oh, you only eat organic?/nyou must be so healthy.',\n",
       " 'oh, you only eat organic?',\n",
       " 'you must be so healthy.',\n",
       " 'https://memegenerator.net/Willy-Wonka/images/popular/alltime/page/1',\n",
       " '0']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "memefile = 'Willy-Wonkamemegenerator.csv' # only 500 unique words fk\n",
    "labels = []\n",
    "with open(memefile) as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for row in spamreader:\n",
    "        labels.append(row)\n",
    "labels.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in labels:\n",
    "#     print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2probdict(alist):\n",
    "    probdict={k:0 for k in alist}\n",
    "    total = len(alist)\n",
    "    for i in alist:\n",
    "        probdict[i]+=1\n",
    "    for i in probdict:\n",
    "        probdict[i] /= total\n",
    "    return probdict\n",
    "\n",
    "def sample_word(dictionary):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for key, value in dictionary.items():\n",
    "        cumulative += value\n",
    "        if p0 < cumulative:\n",
    "            return key\n",
    "\n",
    "class markov_model:\n",
    "    def __init__(self, alistOfLists = [], deg = 2):\n",
    "        self.instances = alistOfLists \n",
    "        if deg > 1:\n",
    "            self.deg = deg\n",
    "        else:\n",
    "            print('deg must be greater than 1, cannot be ', deg)\n",
    "        self.firsts = {} #first item of chain\n",
    "        self.chains = {x:{} for x in tuple(range(1,self.deg+1))} #dictionary of dictionaries of chains (prev):{next1:%, next2:%...}\n",
    "        lemm = WordNetLemmatizer()\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "#for lemmatizing each text correctly\n",
    "        for i in self.instances:\n",
    "            i = i[0].replace('/n', '')\n",
    "            i = nltk.word_tokenize(i)\n",
    "            for j in i:\n",
    "                j = lemm.lemmatize(j,'n')\n",
    "                j = lemm.lemmatize(j,'a')\n",
    "                j = lemm.lemmatize(j,'v')\n",
    "            # print(i)\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "            if i[0] not in self.firsts:\n",
    "                self.firsts[i[0]] = 0\n",
    "            self.firsts[i[0]] += 1\n",
    "\n",
    "            for j in range(1, self.deg): #define ngrams (prev):next\n",
    "                for k in range(len(i)-j):\n",
    "                    prev = []\n",
    "                    for l in range(j):\n",
    "                        prev.append(i[k+l])\n",
    "                    if tuple(prev) not in self.chains[j]:\n",
    "                        self.chains[j][tuple(prev)] = []\n",
    "                    self.chains[j][tuple(prev)].append(i[k+j])\n",
    "\n",
    "        #convert dictionary next to probability\n",
    "        for i in self.firsts:\n",
    "            self.firsts[i] /= len(self.firsts)\n",
    "\n",
    "        for i in self.chains:\n",
    "            for j in self.chains[i]:\n",
    "                self.chains[i][j] = list2probdict(self.chains[i][j])\n",
    "\n",
    "    def chain(self): #creates markov chain for up to degree\n",
    "        sentence = []\n",
    "        word0 = sample_word(self.firsts)\n",
    "        sentence.append(word0)\n",
    "        key = []\n",
    "        word1 = ''\n",
    "        for i in range(1,self.deg):\n",
    "            key.append(sentence[i-1])\n",
    "            while tuple(key) not in self.chains[i]:\n",
    "                key.pop(0)\n",
    "                i -= 1\n",
    "            word1 = sample_word(self.chains[i][tuple(key)])\n",
    "            sentence.append(word1)\n",
    "        while word1 != '.':\n",
    "            order = self.deg - 1\n",
    "            key = sentence[-order:]\n",
    "            while tuple(key) not in self.chains[order]:\n",
    "                key.pop(0)\n",
    "                order -= 1\n",
    "\n",
    "            word1 = sample_word(self.chains[order][tuple(key)])\n",
    "            id = word1.find('/n')\n",
    "            if id != -1: #assuming /n always at the start of a word\n",
    "                word2 = '/n'\n",
    "                word3 = word1[2:]\n",
    "                sentence.append(word2)\n",
    "                sentence.append(word3)\n",
    "            else:\n",
    "                sentence.append(word1)\n",
    "        return sentence\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = markov_model(labels, 4)\n",
    "# for i in a.chains:\n",
    "#     print(i)\n",
    "#     print(a.chains[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def make_path_appropriate(line):\n",
    "    for i in ['*', '.', '\\\"', '/', '\\\\', '[', ']', ':', ';', '|', ',','?']:\n",
    "        line = line.replace(i, '_')\n",
    "    return line\n",
    "\n",
    "def drawline(pos, text, font, draw):\n",
    "    limit = 25\n",
    "    offset = 15\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    line = ''\n",
    "    for i in range(len(tokens)):\n",
    "        line += ' ' + tokens[i]\n",
    "        if len(line)>limit or i == len(tokens)-1:\n",
    "            draw.text(xy=pos, text = line, font = font)            \n",
    "            line = ' '\n",
    "            pos[1] += offset\n",
    "    return draw\n",
    "\n",
    "def makelabel(mm, deg):\n",
    "    label = ' '.join(mm.chain())\n",
    "\n",
    "    if '?' in label:\n",
    "        line1, line2 = label.split('?',1)\n",
    "    else:\n",
    "        line1 = label\n",
    "        line2 = ''\n",
    "    line1 += '?'\n",
    "    return line1, line2, label\n",
    "\n",
    "def splitlines(label):\n",
    "    if '?' in label:\n",
    "        line1, line2 = label.split('?',1)\n",
    "    else:\n",
    "        line1 = label\n",
    "        line2 = ''\n",
    "    line1 += '?'\n",
    "    return line1, line2, label\n",
    "\n",
    "def makememe(line1, line2):\n",
    "    template = r\"Z:\\sibroot\\repo\\personal\\AIMemeGenerator\\meme templates\\220px-Gene_Wilder_as_Willy_Wonka.jpeg\"\n",
    "    font = ImageFont.truetype('impact.ttf', size=15)\n",
    "    im = Image.open(template)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    xy1 = [0,0]\n",
    "    xy2 = [0,130]\n",
    "    \n",
    "    draw = drawline(xy1, line1, font, draw)\n",
    "    draw = drawline(xy2, line2, font, draw)\n",
    "    folder = os.path.join(r\"meme outputs\",'WillyWonka')\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "    output = str(hash(line1+line2))\n",
    "    output = os.path.join(folder, output+'.jpeg')\n",
    "    print(line1)\n",
    "    print(line2)\n",
    "    print(output)\n",
    "    print()\n",
    "    im.show()\n",
    "    im = im.save(output,format = 'jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.013503599999978633s has elapsed\n"
     ]
    }
   ],
   "source": [
    "import timeit\r\n",
    "start = timeit.timeit()\r\n",
    "mm = markov_model(labels, 4)\r\n",
    "end = timeit.timeit()\r\n",
    "\r\n",
    "print('{}s has elapsed'.format(end-start))"
   ]
  },
  {
   "source": [
    "# Result: Setting n = 5 gives coherent phrases that is very similar to original memes due to small dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1, line2, label = makelabel(mm, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makememe(line1, line2)"
   ]
  },
  {
   "source": [
    "def simp_prompt_replacer(prompts, mm, deg):\n",
    "    pos_prompts = nltk.pos_tag(nltk.word_tokenize(nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(prompts)))\n",
    "    found = -1\n",
    "    indexs = []\n",
    "    # while found < 0:\n",
    "    while len(indexs) != len(pos_prompts):\n",
    "        line1, line2, label = makelabel(mm,4)\n",
    "        while '?' not in label:\n",
    "            line1, line2, label = makelabel(mm,4)            \n",
    "        pos_tag = nltk.pos_tag(nltk.word_tokenize(label))\n",
    "        words, pos_tag = list(zip(*pos_tag))\n",
    "        for i in pos_prompts:\n",
    "            if i[1] in pos_tag:\n",
    "                found = list(pos_tag).index(i[1])\n",
    "                indexs.append(list(pos_tag).index(i[1]))\n",
    "            else:\n",
    "                found = -1\n",
    "                indexs = []                    \n",
    "        \n",
    "        words = list(words)\n",
    "        for i in range(len(indexs)):\n",
    "            words[indexs[i]] = pos_prompts[i][0]\n",
    "        label = nltk.tokenize.treebank.TreebankWordDetokenizer().detokenize(words)\n",
    "    qmark = label.count('?')\n",
    "    line1 = label\n",
    "    line2 = ''\n",
    "    lines = []\n",
    "    if qmark > 0:\n",
    "        for i in label.split('?'):\n",
    "            lines.append(i)\n",
    "        line1 = lines[0]\n",
    "        line1 += '?'\n",
    "        line2 = lines[-1]\n",
    "    print(line1, line2)\n",
    "    return line1, line2, label\n",
    "# prompt = input('enter your prompt')\n",
    "# does not work for multiple prompts that are the same \n",
    "prompt = ['dick']\n",
    "line1, line2, label = simp_prompt_replacer(prompt, mm, 4)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['you', 'took', 'intro', 'to', 'business', '?', 'tell', 'me', 'more', 'about', 'how', 'those', 'supporters', 'care', '...', '.']\n[2, 2, 0]\n['me', 'took', 'weenie', 'to', 'business', '?', 'tell', 'me', 'more', 'about', 'how', 'those', 'supporters', 'care', '...', '.']\n\nme took weenie to business?  tell me more about how those supporters care....\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "you have a loud dick?\n you must have been popular with girls.\nmeme outputs\\WillyWonka\\1629198192269224998.jpeg\n\n"
     ]
    }
   ],
   "source": [
    "makememe(line1, line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}